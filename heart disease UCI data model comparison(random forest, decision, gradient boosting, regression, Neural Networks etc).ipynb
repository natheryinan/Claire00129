{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "\n",
    "# tuning and comparison of the popular models with Heart Disease UCI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build of the popular models and tuned models (optimized)\n",
    "\n",
    "Comparison of the optimal for each type models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/2f/aae19e2173c10a9bb7fee5f5cad35dbe53a393960fc91abc477dcc4661e8/pandas-profiling-2.3.0.tar.gz (127kB)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas-profiling) (0.24.2)\n",
      "Requirement already satisfied: matplotlib>=1.4 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas-profiling) (3.1.1)\n",
      "Requirement already satisfied: jinja2>=2.8 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.10.1)\n",
      "Collecting missingno>=0.4.2 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/de/6e4dd6d720c49939544352155dc06a08c9f7e4271aa631a559dfbeaaf9d4/missingno-0.4.2-py3-none-any.whl\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
      "Collecting phik>=0.9.8 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/45/ad/24a16fa4ba612fb96a3c4bb115a5b9741483f53b66d3d3afd987f20fa227/phik-0.9.8-py3-none-any.whl (606kB)\n",
      "Collecting confuse>=1.0.0 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/6f/90e860cba937c174d8b3775729ccc6377eb91f52ad4eeb008e7252a3646d/confuse-1.0.0.tar.gz\n",
      "Requirement already satisfied: astropy in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas-profiling) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas>=0.19->pandas-profiling) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas>=0.19->pandas-profiling) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pandas>=0.19->pandas-profiling) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from matplotlib>=1.4->pandas-profiling) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from matplotlib>=1.4->pandas-profiling) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from missingno>=0.4.2->pandas-profiling) (0.9.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from missingno>=0.4.2->pandas-profiling) (1.2.1)\n",
      "Requirement already satisfied: jupyter-client>=5.2.3 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (5.3.1)\n",
      "Requirement already satisfied: pytest>=4.0.2 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (5.0.1)\n",
      "Requirement already satisfied: nbconvert>=5.3.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (5.5.0)\n",
      "Requirement already satisfied: numba>=0.38.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (0.44.1)\n",
      "Collecting pytest-pylint>=0.13.0 (from phik>=0.9.8->pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/64/dc/6f35f114844fb12e38d60c4f3d2441a55baff7043ad4e013777dff55746c/pytest_pylint-0.14.1-py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from confuse>=1.0.0->pandas-profiling) (5.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.19->pandas-profiling) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (41.0.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (18.0.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.5.0)\n",
      "Requirement already satisfied: traitlets in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.3.2)\n",
      "Requirement already satisfied: tornado>=4.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (6.0.3)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (7.0.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.17)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.4.1)\n",
      "Requirement already satisfied: pygments in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.4.2)\n",
      "Requirement already satisfied: nbformat>=4.4 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.6.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.1.0)\n",
      "Requirement already satisfied: testpath in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.3)\n",
      "Requirement already satisfied: llvmlite>=0.29.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling) (0.29.0)\n",
      "Requirement already satisfied: pylint>=1.4.5 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from traitlets->jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from traitlets->jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.12->pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.0.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
      "Requirement already satisfied: astroid<3,>=2.2.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.2.5)\n",
      "Requirement already satisfied: isort<5,>=4.2.5 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (4.3.21)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (0.6.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.14.11)\n",
      "Requirement already satisfied: lazy-object-proxy in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.4.1)\n",
      "Collecting typed-ast>=1.3.0; implementation_name == \"cpython\" (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/47/a1/7a24868c15d84ed7446106d6c3d73807f58232a695452c0a29679e5a1523/typed_ast-1.4.0-cp37-cp37m-win_amd64.whl (155kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\yinan\\anaconda3\\lib\\site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.11.2)\n",
      "Building wheels for collected packages: pandas-profiling, htmlmin, confuse\n",
      "  Building wheel for pandas-profiling (setup.py): started\n",
      "  Building wheel for pandas-profiling (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Yinan\\AppData\\Local\\pip\\Cache\\wheels\\ce\\c7\\f1\\dbfef4848ebb048cb1d4a22d1ed0c62d8ff2523747235e19fe\n",
      "  Building wheel for htmlmin (setup.py): started\n",
      "  Building wheel for htmlmin (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Yinan\\AppData\\Local\\pip\\Cache\\wheels\\43\\07\\ac\\7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
      "  Building wheel for confuse (setup.py): started\n",
      "  Building wheel for confuse (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Yinan\\AppData\\Local\\pip\\Cache\\wheels\\b0\\b2\\96\\2074eee7dbf7b7df69d004c9b6ac4e32dad04fb7666cf943bd\n",
      "Successfully built pandas-profiling htmlmin confuse\n",
      "Installing collected packages: missingno, htmlmin, pytest-pylint, phik, confuse, pandas-profiling, typed-ast\n",
      "Successfully installed confuse-1.0.0 htmlmin-0.1.12 missingno-0.4.2 pandas-profiling-2.3.0 phik-0.9.8 pytest-pylint-0.14.1 typed-ast-1.4.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c0d29e299135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# NN models\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# model tuning\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download datasets <a class=\"anchor\" id=\"2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA <a class=\"anchor\" id=\"3\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'target'\n",
    "data_target = data[target_name]\n",
    "data = data.drop([target_name], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split data on 30% in the test dataset, the remaining 70% - in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, target, target_test = train_test_split(data, data_target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% split training set to validation set\n",
    "Xtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning models and test for all features <a class=\"anchor\" id=\"5\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n",
    "\n",
    "- Logistic Regression\n",
    "- Support Vector Machines and Linear SVC\n",
    "- KNN or k-Nearest Neighbors\n",
    "- Naive Bayes Classifier or Gaussian Naive Bayes\n",
    "- Decision Tree Classifier, Random Forest, XGB Classifier, LGBM Classifier, ExtraTreesClassifier\n",
    "- Perceptron, Neural Networks with different archictures (Deep Learning)\n",
    "- VotingClassifier (hard or soft voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "Note the confidence score generated by the model based on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train, target)\n",
    "acc_log = round(logreg.score(train, target) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_log = round(logreg.score(test, target_test) * 100, 2)\n",
    "acc_test_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n",
    "\n",
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n",
    "\n",
    "- *cp* is highest negative coefficient\n",
    "- *trestbps*, *thal*, *oldpeak* are largest numbers by absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(train.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines** are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(train, target)\n",
    "acc_svc = round(svc.score(train, target) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_svc = round(svc.score(test, target_test) * 100, 2)\n",
    "acc_test_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC** is a similar to SVM method. Its also builds on kernel functions but is appropriate for unsupervised learning. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support-vector_machine#Support-vector_clustering_(SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "\n",
    "linear_svc = LinearSVC(dual=False)  # dual=False when n_samples > n_features.\n",
    "linear_svc.fit(train, target)\n",
    "acc_linear_svc = round(linear_svc.score(train, target) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_linear_svc = round(linear_svc.score(test, target_test) * 100, 2)\n",
    "acc_test_linear_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pattern recognition, the **k-Nearest Neighbors algorithm** (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors algorithm\n",
    "\n",
    "knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [2, 3]}, cv=10).fit(train, target)\n",
    "acc_knn = round(knn.score(train, target) * 100, 2)\n",
    "print(acc_knn, knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_knn = round(knn.score(test, target_test) * 100, 2)\n",
    "acc_test_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, **Naive Bayes classifiers** are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(train, target)\n",
    "acc_gaussian = round(gaussian.score(train, target) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_gaussian = round(gaussian.score(test, target_test) * 100, 2)\n",
    "acc_test_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Perceptron** is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(train, target)\n",
    "acc_perceptron = round(perceptron.score(train, target) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_perceptron = round(perceptron.score(test, target_test) * 100, 2)\n",
    "acc_test_perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic gradient descent** (often abbreviated **SGD**) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in big data applications this reduces the computational burden, achieving faster iterations in trade for a slightly lower convergence rate. Reference [Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(train, target)\n",
    "acc_sgd = round(sgd.score(train, target) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_sgd = round(perceptron.score(test, target_test) * 100, 2)\n",
    "acc_test_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a **Decision Tree** as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(train, target)\n",
    "acc_decision_tree = round(decision_tree.score(train, target) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_decision_tree = round(decision_tree.score(test, target_test) * 100, 2)\n",
    "acc_test_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests** is one of the most popular model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators= [100, 300]) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(train, target)\n",
    "random_forest.fit(train, target)\n",
    "acc_random_forest = round(random_forest.score(train, target) * 100, 2)\n",
    "print(acc_random_forest,random_forest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_random_forest = round(random_forest.score(test, target_test) * 100, 2)\n",
    "acc_test_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tuning the hyperparameters of the XGBClassifier model using the HyperOpt and 10-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_xgb_score(params):\n",
    "    clf = XGBClassifier(**params)\n",
    "    current_score = cross_val_score(clf, train, target, cv=10).mean()\n",
    "    print(current_score, params)\n",
    "    return current_score \n",
    " \n",
    "space_xgb = {\n",
    "            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n",
    "            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n",
    "            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n",
    "            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n",
    "            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n",
    "            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n",
    "            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n",
    "            'eval_metric': 'auc',\n",
    "            'objective': 'binary:logistic',\n",
    "            'booster': 'gbtree',\n",
    "            'tree_method': 'exact',\n",
    "            'silent': 1,\n",
    "            'missing': None\n",
    "        }\n",
    " \n",
    "best = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = space_eval(space_xgb, best)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_Classifier = XGBClassifier(**params)\n",
    "XGB_Classifier.fit(train, target)\n",
    "acc_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\n",
    "acc_XGB_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_XGB_Classifier = round(XGB_Classifier.score(test, target_test) * 100, 2)\n",
    "acc_test_XGB_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =  plt.figure(figsize = (15,15))\n",
    "axes = fig.add_subplot(111)\n",
    "xgb.plot_importance(XGB_Classifier,ax = axes,height =0.5)\n",
    "plt.show();\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. Reference [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tuning the hyperparameters of the LGBMClassifier model using the HyperOpt and 10-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_lgb_score(params):\n",
    "    clf = LGBMClassifier(**params)\n",
    "    current_score = cross_val_score(clf, train, target, cv=10).mean()\n",
    "    print(current_score, params)\n",
    "    return current_score \n",
    " \n",
    "space_lgb = {\n",
    "            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n",
    "            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n",
    "            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n",
    "            'num_leaves': hp.choice('num_leaves', 2*np.arange(2, 2**11, dtype=int)),\n",
    "            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n",
    "            'objective': 'binary',\n",
    "            'boosting_type': 'gbdt',\n",
    "            }\n",
    " \n",
    "best = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = space_eval(space_lgb, best)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_Classifier = LGBMClassifier(**params)\n",
    "LGB_Classifier.fit(train, target)\n",
    "acc_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\n",
    "acc_LGB_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_LGB_Classifier = round(LGB_Classifier.score(test, target_test) * 100, 2)\n",
    "acc_test_LGB_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =  plt.figure(figsize = (15,15))\n",
    "axes = fig.add_subplot(111)\n",
    "lgb.plot_importance(LGB_Classifier,ax = axes,height = 0.5)\n",
    "plt.show();\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting** builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_gb_score(params):\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    current_score = cross_val_score(clf, train, target, cv=10).mean()\n",
    "    print(current_score, params)\n",
    "    return current_score \n",
    " \n",
    "space_gb = {\n",
    "            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n",
    "            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int))            \n",
    "        }\n",
    " \n",
    "best = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = space_eval(space_gb, best)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "gradient_boosting = GradientBoostingClassifier(**params)\n",
    "gradient_boosting.fit(train, target)\n",
    "acc_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\n",
    "acc_gradient_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_gradient_boosting = round(gradient_boosting.score(test, target_test) * 100, 2)\n",
    "acc_test_gradient_boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tikhonov Regularization, colloquially known as **Ridge Regression**, is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution. This type of problem is very common in machine learning tasks, where the \"best\" solution must be chosen using limited data. If a unique solution exists, algorithm will return the optimal value. However, if multiple solutions exist, it may choose any of them. Reference [Brilliant.org](https://brilliant.org/wiki/ridge-regression/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Classifier\n",
    "\n",
    "ridge_classifier = RidgeClassifier()\n",
    "ridge_classifier.fit(train, target)\n",
    "acc_ridge_classifier = round(ridge_classifier.score(train, target) * 100, 2)\n",
    "acc_ridge_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_ridge_classifier = round(ridge_classifier.score(test, target_test) * 100, 2)\n",
    "acc_test_ridge_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap aggregating, also called **bagging**, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach. Bagging leads to \"improvements for unstable procedures\", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors. Reference [Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating).\n",
    "\n",
    "A **Bagging classifier** is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Classifier\n",
    "\n",
    "bagging_classifier = BaggingClassifier()\n",
    "bagging_classifier.fit(train, target)\n",
    "Y_pred = bagging_classifier.predict(test).astype(int)\n",
    "acc_bagging_classifier = round(bagging_classifier.score(train, target) * 100, 2)\n",
    "acc_bagging_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_bagging_classifier = round(bagging_classifier.score(test, target_test) * 100, 2)\n",
    "acc_test_bagging_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ExtraTreesClassifier** implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The default values for the parameters controlling the size of the trees (e.g. max_depth, min_samples_leaf, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html). \n",
    "\n",
    "In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/ensemble.html#Extremely%20Randomized%20Trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_etc_score(params):\n",
    "    clf = ExtraTreesClassifier(**params)\n",
    "    current_score = cross_val_score(clf, train, target, cv=10).mean()\n",
    "    print(current_score, params)\n",
    "    return current_score \n",
    " \n",
    "space_etc = {\n",
    "            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n",
    "            'max_features': hp.choice('max_features', np.arange(2, 17, dtype=int)),\n",
    "            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 5, dtype=int)),\n",
    "            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n",
    "            'max_features': None # for small number of features\n",
    "        }\n",
    " \n",
    "best = fmin(fn=hyperopt_etc_score, space=space_etc, algo=tpe.suggest, max_evals=10)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = space_eval(space_etc, best)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classifier\n",
    "\n",
    "extra_trees_classifier = ExtraTreesClassifier(**params)\n",
    "extra_trees_classifier.fit(train, target)\n",
    "acc_etc = round(extra_trees_classifier.score(train, target) * 100, 2)\n",
    "acc_etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_etc = round(extra_trees_classifier.score(test, target_test) * 100, 2)\n",
    "acc_test_etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural networks** are more complex and more powerful algorithm than standars machine learning, it belongs to deep learning models. To build a neural network use Keras. Keras is a high level API for tensorflow, which is a tensor-manipulation framework made by google. Keras allows you to build neural networks by assembling blocks (which are the layers of neural network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann(optimizer='adam'):\n",
    "    \n",
    "    # Initializing the ANN\n",
    "    ann = Sequential()\n",
    "    \n",
    "    # Adding the input layer and the first hidden layer of the ANN with dropout\n",
    "    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(len(train.columns),)))\n",
    "    \n",
    "    # Add other layers, it is not necessary to pass the shape because there is a layer before\n",
    "    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    ann.add(Dropout(rate=0.5))\n",
    "    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    ann.add(Dropout(rate=0.5))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "    \n",
    "    # Compiling the ANN\n",
    "    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001)\n",
    "ann = build_ann(opt)\n",
    "# Training the ANN\n",
    "history = ann.fit(Xtrain, Ztrain, batch_size=16, epochs=100, validation_data=(Xval, Zval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Train set results\n",
    "ann_prediction = ann.predict(train)\n",
    "ann_prediction = (ann_prediction > 0.5)*1 # convert probabilities to binary output\n",
    "\n",
    "# Compute error between predicted data and true response and display it in confusion matrix\n",
    "acc_ann1 = round(metrics.accuracy_score(target, ann_prediction) * 100, 2)\n",
    "acc_ann1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "ann_prediction_test = ann.predict(test)\n",
    "ann_prediction_test = (ann_prediction_test > 0.5)*1 # convert probabilities to binary output\n",
    "\n",
    "# Compute error between predicted data and true response and display it in confusion matrix\n",
    "acc_test_ann1 = round(metrics.accuracy_score(target_test, ann_prediction_test) * 100, 2)\n",
    "acc_test_ann1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim = train.shape[1], init = 'he_normal', activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, init = 'he_normal', activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, init = 'he_normal', activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', patience=20, mode='max')\n",
    "hist = model.fit(train, target, batch_size=64, validation_data=(Xval, Zval), \n",
    "               epochs=500, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='acc')\n",
    "plt.plot(hist.history['val_accuracy'], label='val_acc')\n",
    "# plt.plot(hist.history['acc'], label='acc')\n",
    "# plt.plot(hist.history['val_acc'], label='val_acc')\n",
    "plt.ylim((0, 1))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Train set results\n",
    "nn_prediction = model.predict(train)\n",
    "nn_prediction = (nn_prediction > 0.5)*1 # convert probabilities to binary output\n",
    "\n",
    "# Compute error between predicted data and true response\n",
    "acc_ann2 = round(metrics.accuracy_score(target, nn_prediction) * 100, 2)\n",
    "acc_ann2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "nn_prediction_test = model.predict(test)\n",
    "nn_prediction_test = (nn_prediction_test > 0.5)*1 # convert probabilities to binary output\n",
    "\n",
    "# Compute error between predicted data and true response\n",
    "acc_test_ann2 = round(metrics.accuracy_score(target_test, nn_prediction_test) * 100, 2)\n",
    "acc_test_ann2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core principle of **AdaBoost** is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples. Initially, those weights are all set to 1/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_ab_score(params):\n",
    "    clf = AdaBoostClassifier(**params)\n",
    "    current_score = cross_val_score(clf, train, target, cv=10).mean()\n",
    "    print(current_score, params)\n",
    "    return current_score \n",
    " \n",
    "space_ab = {\n",
    "            'n_estimators': hp.choice('n_estimators', range(50, 1000)),\n",
    "            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001)       \n",
    "        }\n",
    " \n",
    "best = fmin(fn=hyperopt_ab_score, space=space_ab, algo=tpe.suggest, max_evals=10)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = space_eval(space_ab, best)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "\n",
    "Ada_Boost = AdaBoostClassifier(**params)\n",
    "Ada_Boost.fit(train, target)\n",
    "Ada_Boost.score(train, target)\n",
    "acc_AdaBoost = round(Ada_Boost.score(train, target) * 100, 2)\n",
    "acc_AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_AdaBoost = round(Ada_Boost.score(test, target_test) * 100, 2)\n",
    "acc_test_AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the **VotingClassifier** is to combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/ensemble.html#Voting%20Classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VotingClassifier (with **hard voting**) would classify the sample as “class 1” based on the **majority class label**. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/ensemble.html#Voting%20Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_Classifier_hard = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('ab', Ada_Boost)], voting='hard')\n",
    "for clf, label in zip([logreg, random_forest, Ada_Boost, Voting_Classifier_hard], \n",
    "                      ['Logistic Regression', 'Random Forest', 'AdaBoost Classifier', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, train, target, cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_Classifier_hard.fit(train, target)\n",
    "acc_VC_hard = round(Voting_Classifier_hard.score(train, target) * 100, 2)\n",
    "acc_VC_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_VC_hard = round(Voting_Classifier_hard.score(test, target_test) * 100, 2)\n",
    "acc_test_VC_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to majority voting (hard voting), **soft voting** returns the class label as argmax of the **sum of predicted probabilities**.\n",
    "Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. Reference [sklearn documentation](https://scikit-learn.org/stable/modules/ensemble.html#Voting%20Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('ab', Ada_Boost)], voting='soft')\n",
    "params = {'lr__C': [1.0, 100.0], 'ab__learning_rate': [0.0001, 1]}\n",
    "Voting_Classifier_soft = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "Voting_Classifier_soft.fit(train, target)\n",
    "acc_VC_soft = round(Voting_Classifier_soft.score(train, target) * 100, 2)\n",
    "acc_VC_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_VC_soft = round(Voting_Classifier_soft.score(test, target_test) * 100, 2)\n",
    "acc_test_VC_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', 'k-Nearest Neighbors', 'Naive Bayes', \n",
    "              'Perceptron', 'Stochastic Gradient Decent', \n",
    "              'Decision Tree Classifier', 'Random Forest',  'XGBClassifier', 'LGBMClassifier',\n",
    "              'GradientBoostingClassifier', 'RidgeClassifier', 'BaggingClassifier', 'ExtraTreesClassifier', \n",
    "              'Neural Network 1', 'Neural Network 2', \n",
    "              'VotingClassifier-hard voiting', 'VotingClassifier-soft voting',\n",
    "              'AdaBoostClassifier'],\n",
    "    \n",
    "    'Score_train': [acc_log, acc_svc, acc_linear_svc, acc_knn, acc_gaussian, \n",
    "              acc_perceptron, acc_sgd, \n",
    "              acc_decision_tree, acc_random_forest, acc_XGB_Classifier, acc_LGB_Classifier,\n",
    "              acc_gradient_boosting, acc_ridge_classifier, acc_bagging_classifier, acc_etc, \n",
    "              acc_ann1, acc_ann2, \n",
    "              acc_VC_hard, acc_VC_soft,\n",
    "              acc_AdaBoost],\n",
    "    'Score_test': [acc_test_log, acc_test_svc, acc_test_linear_svc, acc_test_knn, acc_test_gaussian, \n",
    "              acc_test_perceptron, acc_test_sgd, \n",
    "              acc_test_decision_tree, acc_test_random_forest, acc_test_XGB_Classifier, acc_test_LGB_Classifier,\n",
    "              acc_test_gradient_boosting, acc_test_ridge_classifier, acc_test_bagging_classifier, acc_test_etc, \n",
    "              acc_test_ann1, acc_test_ann2, \n",
    "              acc_test_VC_hard, acc_test_VC_soft,\n",
    "              acc_test_AdaBoost]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort_values(by=['Score_train', 'Score_test'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort_values(by=['Score_test', 'Score_train'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Score_diff'] = abs(models['Score_train'] - models['Score_test'])\n",
    "models.sort_values(by=['Score_diff'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=[25,6])\n",
    "xx = models['Model']\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.plot(xx, models['Score_train'], label = 'Score_train')\n",
    "plt.plot(xx, models['Score_test'], label = 'Score_test')\n",
    "plt.legend()\n",
    "plt.title('Score of 20 popular models for train and test datasets')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score, %')\n",
    "plt.xticks(xx, rotation='vertical')\n",
    "plt.savefig('graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion <a class=\"anchor\" id=\"7\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The worst models by criterion *Score_train* are **Perceptron, Neural Networks, Stochastic Gradient Decent, k-Nearest Neighbors** - these models are worst suited to the training data\n",
    "\n",
    "#The best models by criterion *Score_test* are **AdaBoostClassifier, Random Forest, BaggingClassifier, Logistic Regression, Naive Bayes** - these models have best learned how to predict test or other data\n",
    "#The best models by criterion *Score_diff* are **RidgeClassifier, Logistic Regression, Linear SVC, Naive Bayes** - these models have the most stable accuracy, that is, the smallest difference between the predictions of both training and test data\n",
    "\n",
    "#The models are optimal by all criteria: **Random Forest, AdaBoostClassifier, Logistic Regression, Naive Bayes**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
